{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多标签车牌识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as torch_utils_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIGITS = \"0123456789\"\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "#PROVINCE=\"黑吉辽京津内冀鲁豫徽苏沪浙赣闽粤鄂湘云贵川渝藏青宁新陕甘宁晋\" #30\n",
    "CHARS = LETTERS + DIGITS\n",
    "NPLEN=7\n",
    "NUM_CLASSES=1+len(CHARS)*NPLEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv=nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=3,padding=1), #layer1, inputs single channel,224*224\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), #layer2 inputs 64 channel,112*112\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1), #layer3 inputs 128 channel,56*56\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1), #layer4 inputs 256 channel,28*28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1), #layer5 inputs 512 channel,14*14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "    )\n",
    "\n",
    "class vgg16train(nn.Module):\n",
    "    def __init__(self): #36*7+1=253   36*6+1=217\n",
    "        super(vgg16train,self).__init__()\n",
    "        self.features=conv\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "        #initialize_weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class vgg16detect(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(vgg16detect,self).__init__()\n",
    "        self.features=conv\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Conv2d(512,4096,kernel_size=7,padding=1),  #padding=1?\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(4096,2048,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(4096,num_classes,kernel_size=1,padding=1),\n",
    "            #nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self,x):  #是否需要\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.classifier(x)\n",
    "        return x    \n",
    "\n",
    "class anprmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(anprmodel,self).__init__()\n",
    "        self.num_classes=NUM_CLASSES\n",
    "        self.conv1=nn.Conv2d(1,48,kernel_size=5,padding=2)\n",
    "        self.pool1=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2=nn.Conv2d(48,64,kernel_size=5,padding=2)\n",
    "        self.pool2=nn.MaxPool2d(kernel_size=(2,1),stride=(2,1))      #(kH,kW)\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=5,padding=2)\n",
    "        self.pool3=nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))        \n",
    "        self.fc1=nn.Linear(32*8*128,2048)\n",
    "        self.fc2=nn.Linear(2048,NUM_CLASSES)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        x=F.relu(self.pool1(self.conv1(x)))  #224*224  128*64\n",
    "        x=F.relu(self.pool2(self.conv2(x)))  #112*112   64*32\n",
    "        x=F.relu(self.pool3(self.conv3(x)))  #56*56       64*16\n",
    "        x=x.view(-1,32*8*128)                        #32*8\n",
    "        #x=x.view(-1,28*28*128)                   #28*28\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)                                       #253\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NPSET(torch_utils_data.Dataset):\n",
    "    picroot='np'\n",
    "   \n",
    "    def code_to_vec(self,p, code):\n",
    "        def char_to_vec(c):\n",
    "            y = np.zeros((len(CHARS),))\n",
    "            y[CHARS.index(c)] = 1.0\n",
    "            return y\n",
    "        c = np.vstack([char_to_vec(c) for c in code])\n",
    "        return np.concatenate([[1. if p else 0], c.flatten()])\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        label,img=self.labels[index], self.dataset[index]\n",
    "        if self.data_transform is not None:\n",
    "            img=self.data_transform(img)\n",
    "        labelarray=self.code_to_vec(1,label)\n",
    "        #if self.label_transform is not None:\n",
    "        #    labelarray=self.label_transform(labelarray)\n",
    "        return img,labelarray\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __init__(self,root,data_transform=None):\n",
    "        self.picroot=root\n",
    "        self.data_transform=data_transform\n",
    "\n",
    "        if not os.path.exists(self.picroot):\n",
    "            raise RuntimeError('{} doesnot exists'.format(self.picroot))\n",
    "        for root,dnames,filenames in os.walk(self.picroot):\n",
    "            imgs=np.ndarray(shape=(len(filenames),1,64,128),dtype=np.float)  #batch,channel,height,width\n",
    "            labels=[]\n",
    "            i=0\n",
    "            for filename in filenames:\n",
    "                picfilename=os.path.join(self.picroot,filename)  #file name:\n",
    "                im=cv2.imread(picfilename,cv2.IMREAD_GRAYSCALE)\n",
    "                #im=cv2.resize(im,(224,224))\n",
    "                #(thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "                #im=cv2.erode(im,self.kernel)\n",
    "                #im=cv2.dilate(im,self.kernel)\n",
    "                #im=cv2.GaussianBlur(im,(5,5),0.1)\n",
    "                #(thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "                imgs[i][0]=im/255\n",
    "                m=filename.split('_')  #filename style: xxxxxxxx_xxxxxxx_x.png\n",
    "                labels.append(m[1])\n",
    "                i=i+1\n",
    "            self.dataset=imgs\n",
    "            self.labels=labels\n",
    "            self.len=len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=anprmodel()\n",
    "#model.features=torch.nn.DataParallel(model.features)\n",
    "#model.cuda()\n",
    "#cudnn.benchmark=True\n",
    "batch_size=4\n",
    "data_transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "npset = NPSET(root='/home/wang/git/anpr/np', data_transform=data_transform)\n",
    "nploader = torch.utils.data.DataLoader(npset, batch_size=batch_size, shuffle=False, num_workers=1)  #train\n",
    "npvalset=NPSET(root='/home/wang/git/anpr/npval', data_transform=data_transform)\n",
    "npvalloader=torch.utils.data.DataLoader(npvalset, batch_size=batch_size, shuffle=False, num_workers=1) #validate\n",
    "criterion=nn.MultiLabelMarginLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),0.1,momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_sum=0\n",
    "res_cnt=0\n",
    "res_avg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0,1):\n",
    "    #Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
    "    lr=0.1*(0.1**(epoch//30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=lr\n",
    "    #train\n",
    "    model.train()\n",
    "    for i,data in enumerate(nploader):\n",
    "        inputs,targets = data\n",
    "        #target=target.cuda()\n",
    "        input_var=torch.autograd.Variable(inputs)\n",
    "        targets=torch.LongTensor(np.array(targets.numpy(),np.long))\n",
    "        target_var=torch.autograd.Variable(targets)\n",
    "        output=model(input_var)\n",
    "        #porcess loss\n",
    "        loss=criterion(output,target_var)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #\n",
    "        if i% 12 == 0:\n",
    "             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
    "                  epoch, i * len(data), len(nploader.dataset),\n",
    "                  100. * i / len(nploader), loss.data[0]))\n",
    "        \n",
    "    #validate\n",
    "    model.eval()\n",
    "    for i, data in enumerate(npvalloader):\n",
    "        (inputs, target)=data\n",
    "        #target = target.cuda()\n",
    "        input_var = torch.autograd.Variable(inputs, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        #porcess loss\n",
    "        o=torch.FloatTensor(np.reshape(output.data.numpy()[:,1:],(-1,len(CHARS))))\n",
    "        t=torch.LongTensor(np.array(np.reshape(target_var.data.numpy()[:,1:],(-1,len(CHARS))),np.long))\n",
    "        chararcter_loss=cerition(torch.autograd.Variable(o), torch.autograd.Variable(t))\n",
    "        #\n",
    "        bo=torch.FloatTensor(output.data.numpy()[:,1:])\n",
    "        bt=\n",
    "        b=0,d=0\n",
    "        for k < o.size(0)\n",
    "        #\n",
    "        if i% 12 == 0:\n",
    "             print('Test Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
    "                  epoch, i * len(data), len(nploader.dataset),\n",
    "                  100. * i / len(nploader), loss.data[0]))\n",
    "        prec1=top1.avg\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "    if is_best:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch':vgg16,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/mratsim/starting-kit-for-pytorch-deep-learning\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from torch import np # Torch wrapper for Numpy\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "IMG_PATH = '../input/train-jpg/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = '../input/train.csv'\n",
    "\n",
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['image_name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)\n",
    "        \n",
    "transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n",
    "\n",
    "dset_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n",
    "        \n",
    "\n",
    "\n",
    "train_loader = DataLoader(dset_train,\n",
    "                          batch_size=256,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(2304, 256)\n",
    "        self.fc2 = nn.Linear(256, 17)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1) # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "model = Net() # On CPU\n",
    "#model = Net().cuda() # On GPU\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "    \n",
    "\n",
    "\n",
    "Train Epoch: 1 [0/40479 (0%)]\tLoss: 0.690799\n",
    "Train Epoch: 1 [2560/40479 (6%)]\tLoss: 0.686433\n",
    "Train Epoch: 1 [5120/40479 (13%)]\tLoss: 0.681382\n",
    "Train Epoch: 1 [7680/40479 (19%)]\tLoss: 0.673910\n",
    "Train Epoch: 1 [10240/40479 (25%)]\tLoss: 0.667018\n",
    "Train Epoch: 1 [12800/40479 (31%)]\tLoss: 0.656438\n",
    "Train Epoch: 1 [15360/40479 (38%)]\tLoss: 0.645444\n",
    "Train Epoch: 1 [17920/40479 (44%)]\tLoss: 0.628610\n",
    "Train Epoch: 1 [20480/40479 (50%)]\tLoss: 0.600967\n",
    "Train Epoch: 1 [23040/40479 (57%)]\tLoss: 0.570082\n",
    "Train Epoch: 1 [25600/40479 (63%)]\tLoss: 0.520596\n",
    "Train Epoch: 1 [28160/40479 (69%)]\tLoss: 0.465080\n",
    "Train Epoch: 1 [30720/40479 (75%)]\tLoss: 0.412709\n",
    "Train Epoch: 1 [33280/40479 (82%)]\tLoss: 0.365693\n",
    "Train Epoch: 1 [35840/40479 (88%)]\tLoss: 0.357215\n",
    "Train Epoch: 1 [38400/40479 (94%)]\tLoss: 0.340456\n",
    "\n",
    "    \n",
    "\n",
    "https://discuss.pytorch.org/t/feedback-on-pytorch-for-kaggle-competitions/2252\n",
    "\n",
    "https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/13\n",
    "@AjayTalati\n",
    "\n",
    "Either after your last fc you do a sigmoid and then you use BCELoss or F.binary_crossentropy as your criterion/lossfunction\n",
    "\n",
    "Or you directly use MultiLabelSoftMarginLoss as your loss function (it comes with sigmoid inside)\n",
    "\n",
    "Now once you have your prediction, you need to threshold. 0.5 is the default naive way but it's probably not optimal. In any case, once you get there, great !\n",
    "\n",
    "Next part is technical optimization, you can do Multilabel classification without\n",
    "\n",
    "Regarding the threshold, you might want to optimize either a common threshold for all your outputs (it can be 0.2, 0.5, 0.123456 who knows) or optimize a threshold per label class, especially if your classes as unbalanced.\n",
    "You will need a solid validation set and a MultiLabel evaluation metrics (Hamming Loss, F1-score, Fbeta score).\n",
    "\n",
    "An example code for the first strategy is here on Kaggle2.\n",
    "\n",
    "For the second strategy, I'm deep into various papers myself so I can't help yet.\n",
    "One thing to keep in mind is your \"best threshold\" will probably overfit the validation set, so use regularization, cross-validation or other anti-overfitting strategy.\n",
    "\n",
    "https://discuss.pytorch.org/t/equivalent-of-tensorflows-sigmoid-cross-entropy-with-logits-in-pytorch/1985/11\n",
    "@AjayTalati I managed to use BCELoss, binary_crossentropy and MultiLabelSoftMarginLoss on a MultiLabel problem\n",
    "\n",
    "Here is the basic code\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "And the source is here6.\n",
    "\n",
    "For BCELoss you can use criterion = BCELoss() and then loss = criterion(output, target) but as @Misha_E said, the NN must return a sigmoid activation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
