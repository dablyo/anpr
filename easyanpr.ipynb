{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#车牌图像训练，判断是否车牌图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv=nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=3,padding=1), #layer1, inputs single channel,256*128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), #layer2 inputs 64 channel,128*64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1), #layer3 inputs 128 channel,64*32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1), #layer4 inputs 256 channel,32*16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1), #layer5 inputs 512 channel,16*8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "    )\n",
    "\n",
    "class vgg16train(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg16train,self).__init__()\n",
    "        self.features=conv\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 *3* 8 * 4, 4096),                             #connect: layter: inputs 512, 8*4\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 2)\n",
    "        )\n",
    "        #initialize_weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class npModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(npModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3,padding=1)   \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3,padding=1) \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(20*8*16, 100)   #16*32*20\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))                                #32*64\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))   #16*32\n",
    "        x = x.view(-1, 20*8*16)      \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count    \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)   #pred是索引/位置指示\n",
    "    pred = pred.t()                                                 #转置\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get back picture 生成反例图片/背景图片\n",
    "OUTPUT_SHAPE=(128,256)\n",
    "SRCPICDIR='/home/wang/git/deep-anpr/bgs'\n",
    "DSTPICDIR='/home/wang/git/nppic/nproot/back'\n",
    "SRC_NUM_PIC=10000\n",
    "DST_NUM_PIC=500\n",
    "def generate_bg(num_bg_img):\n",
    "    found=False\n",
    "    while not found :\n",
    "        fname=\"{}/{:08d}.jpg\".format(SRCPICDIR,random.randint(0, SRC_NUM_PIC - 1))\n",
    "        bg=cv2.imread(fname) #,cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "        bg=cv2.cvtColor(bg,cv2.COLOR_BGR2GRAY)\n",
    "        if(bg.shape[0]>=OUTPUT_SHAPE[0]  and bg.shape[1]>=OUTPUT_SHAPE[1]):\n",
    "            found=True\n",
    "    x=random.randint(0,bg.shape[1]-OUTPUT_SHAPE[1])\n",
    "    y=random.randint(0,bg.shape[0]-OUTPUT_SHAPE[0])\n",
    "    return bg[y:y+OUTPUT_SHAPE[0],x:x+OUTPUT_SHAPE[1]]\n",
    "    \n",
    "if os.path.exists(DSTPICDIR):\n",
    "    raise IOError,DSTPICDIR+\" has been exists\" \n",
    "os.mkdir(DSTPICDIR)            \n",
    "i=0\n",
    "while i<600:\n",
    "    #bg=generate_bg(SRC_NUM_PIC)\n",
    "    fname=os.path.join(DSTPICDIR,\"{:4d}.jpg\".format(random.randint(0,9999)))\n",
    "    #cv2.imwrite(fname,generate_bg(SRC_NUM_PIC))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.505113917 3847.16795699\n"
     ]
    }
   ],
   "source": [
    "#mean and variance 得到均值和方差\n",
    "TRAINDIR='/home/wang/git/nppic/nproot'\n",
    "f=[]\n",
    "for parent,dirnames,filenames in os.walk(TRAINDIR):\n",
    "     for i,fname in enumerate(filenames):\n",
    "        f.append(os.path.join(parent,fname))\n",
    "img=np.ndarray((len(f),128,256))\n",
    "for i,fname in enumerate(f):\n",
    "    im=cv2.imread(os.path.join(TRAINDIR,fname))\n",
    "    img[i]=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "mean=np.mean(img)\n",
    "var=np.var(img)\n",
    "print mean,var\n",
    "#不除以255：mean/var:107.505113917 3847.16795699\n",
    "#除以255，mean/var:0.42312632171/0.0598781834145\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def make_scaled_ims(im, min_shape):\n",
    "#    ratio = 1. / 2 ** 0.5\n",
    "#        yield cv2.resize(im, (shape[1], shape[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gray_loader(path):#input 128*256\n",
    "    im=cv2.imread(path)\n",
    "    im=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    #imp=np.ndarray((1,im.shape[0],im.shape[1]),np.uint8)\n",
    "    #imp[0]=im\n",
    "    #im=imp.transpose((1,2,0))\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "data_transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((107.5051,), (3847.1679,)),\n",
    "                             ])\n",
    "npset = datasets.ImageFolder(root='/home/wang/git/nppic/np/nproot', \n",
    "                             transform=data_transform,loader=gray_loader)\n",
    "nploader = torch.utils.data.DataLoader(npset, batch_size=batch_size, shuffle=True, \n",
    "                                       num_workers=1)  #train\n",
    "npvalset=datasets.ImageFolder(root='/home/wang/git/nppic/np/npval', \n",
    "                              transform=data_transform,loader=gray_loader)\n",
    "npvalloader=torch.utils.data.DataLoader(npvalset, batch_size=batch_size, shuffle=True, \n",
    "                                        num_workers=1) #validate\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "model=vgg16train()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CHECK_ARG(input->nDimension == output->nDimension) failed at torch/csrc/cudnn/Conv.cpp:275",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a977348ad633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-699a3297c6a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 237\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/anaconda2/envs/pytorch/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     37\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     38\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CHECK_ARG(input->nDimension == output->nDimension) failed at torch/csrc/cudnn/Conv.cpp:275"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "losses=AverageMeter()\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    losses.reset()\n",
    "    for _, (datas,targets) in enumerate(nploader):\n",
    "        datas=torch.unsqueeze(datas,1)\n",
    "        datas_var,targets_var=torch.autograd.Variable(datas),torch.autograd.Variable(targets)\n",
    "        datas_var=datas_var.cuda()\n",
    "        targets_var=targets_var.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs_var = model(datas_var)\n",
    "        loss = criterion(outputs_var, targets_var)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        #loss\n",
    "        losses.update(loss.data[0],datas_var.data.size(0))  # datas size(0) is batch_size of input\n",
    "        if losses.count % 500 == 0:\n",
    "            print ('[%d, %4d] currnet los: %.5f, ave loss: %.5f' %(epoch+1,losses.count,loss.data[0], losses.avg)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./plate.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 100.00\n"
     ]
    }
   ],
   "source": [
    "#validate\n",
    "prediction=AverageMeter()\n",
    "model.eval()\n",
    "for _, (datas,targets) in enumerate(npvalloader):\n",
    "    datas=torch.unsqueeze(datas,1)\n",
    "    datas_var,targets_var=torch.autograd.Variable(datas),torch.autograd.Variable(targets)\n",
    "    datas_var=datas_var.cuda()\n",
    "    targets_var=targets_var.cuda()    \n",
    "    outputs_var = model(datas_var)\n",
    "    prec=accuracy(outputs_var.data,targets_var.data)   #40% will return 40\n",
    "    prediction.update(prec[0].cpu().numpy()[0],datas_var.data.size(0))\n",
    "    \n",
    "#print(' accuracy is{val.avg:%.2f}'.format(val=prediction))\n",
    "print ('accuracy is: %.2f' %(prediction.avg)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "net=vgg16train()\n",
    "net.load_state_dict(torch.load('/home/wang/git/anpr/plate.weight'))\n",
    "net.cuda()\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plate': 1, 'back': 0}\n"
     ]
    }
   ],
   "source": [
    "nptestset=datasets.ImageFolder(root='/home/wang/git/nppic/npr', \n",
    "                              transform=data_transform,loader=gray_loader)\n",
    "nptestloader=torch.utils.data.DataLoader(npvalset, batch_size=1, shuffle=False, \n",
    "                                        num_workers=1) #validate\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "classes,class_to_idx=find_classes('/home/wang/git/nppic/npr')\n",
    "print class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nptest=iter(nptestloader)\n",
    "datas,targets=nptest.next()\n",
    "targets\n",
    "#targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "datas=torch.unsqueeze(datas,1)\n",
    "datas_var,targets_var=torch.autograd.Variable(datas),torch.autograd.Variable(targets)\n",
    "datas_var=datas_var.cuda()\n",
    "targets_var=targets_var.cuda()   \n",
    "outputs_var=net(datas_var)\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-10.3887   8.9537\n",
       "[torch.cuda.FloatTensor of size 1x2 (GPU 0)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classesc=['back','plate']\n",
    "_,pred=torch.max(outputs_var,1)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "plate\n"
     ]
    }
   ],
   "source": [
    "p=pred.cpu().data.numpy()\n",
    "for i,idx in enumerate(p):\n",
    "    print classesc[idx[0]]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean and variance\n",
    "TRAINDIR='/home/wang/git/nppic/nproot/plate'\n",
    "f=[]\n",
    "for parent,dirnames,filenames in os.walk(TRAINDIR):\n",
    "    for filename in filenames:\n",
    "        f.append(filename)\n",
    "img=np.ndarray((len(f),128,256))\n",
    "for i,fname in enumerate(f):\n",
    "    im=cv2.imread(os.path.join(TRAINDIR,fname))\n",
    "    img[i]=cv2.imread(im,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img=img/255  #[0,1]\n",
    "\n",
    "mean=np.mean(img)   #-0.18262765251608459\n",
    "var=np.var(img)             #0.21571192247129692\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
